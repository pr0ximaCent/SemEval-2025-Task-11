{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DL Models' Pipeline","metadata":{}},{"cell_type":"code","source":"# Importing necessary libraries :)\nimport pandas as pd\nimport numpy as np\nimport os\nimport re\nimport emoji\nimport nltk\nimport tensorflow as tf\nfrom nltk.corpus import stopwords\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, LSTM, Bidirectional, GRU, SpatialDropout1D\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.metrics import f1_score\nfrom tensorflow.keras.utils import to_categorical\nfrom tqdm import tqdm\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, Bidirectional, LSTM, GRU, BatchNormalization","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Downloading stopwords :()\nnltk.download('stopwords')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Defining languages\nlanguages = [\"arq\", \"amh\", \"hau\", \"orm\", \"som\"]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Defining stopwords :) :(\nstopwords_dict={\n    \"arq\": set(stopwords.words('arabic')),\n    \"amh\": set(stopwords.words('english')),  \n    \"hau\": set(stopwords.words('english')),  \n    \"orm\": set(stopwords.words('english')),  \n    \"som\": set(stopwords.words('english')),  \n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Text preprocessing function\ndef preprocess_text(text, lang=\"English\"):\n    # Remove emojis\n    text = emoji.replace_emoji(text, replace=\"\")\n    # Remove punctuation\n    text = re.sub(r'[^\\w\\s]', '', text)\n    words = text.lower().split()\n    if lang in stopwords_dict:\n        words = [word for word in words if word not in stopwords_dict[lang]]\n    \n    return \" \".join(words)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Model training function\ndef train_model(X_train, y_train, X_dev, y_dev, model_type=\"GRU\"):\n    vocab_size = 10000\n    embedding_dim = 128\n    max_length = 100\n    \n    model = Sequential()\n    model.add(Embedding(vocab_size, embedding_dim, input_length=max_length))\n    model.add(SpatialDropout1D(0.2))\n\n    if model_type == \"CNN\":\n        model.add(Conv1D(128, 5, activation='relu'))  # can be used other activation functions\n        model.add(MaxPooling1D(2))\n        model.add(BatchNormalization())\n        model.add(Conv1D(64, 3, activation='relu'))\n        model.add(MaxPooling1D(2))\n        model.add(Dropout(0.3))\n        model.add(Flatten())\n        model.add(Dense(128, activation='relu'))\n        model.add(Dropout(0.3))\n    \n    elif model_type == \"BiLSTM+CNN\":\n        model.add(Bidirectional(LSTM(128, return_sequences=True)))\n        model.add(Dropout(0.3))\n        model.add(Conv1D(128, 5, activation='relu'))\n        model.add(MaxPooling1D(2))\n        model.add(BatchNormalization())\n        model.add(Conv1D(64, 3, activation='relu'))\n        model.add(MaxPooling1D(2))\n        model.add(Dropout(0.3))\n        model.add(Flatten())\n        model.add(Dense(128, activation='relu'))\n        model.add(Dropout(0.3))\n    \n    elif model_type == \"BiLSTM+BiGRU\":\n        model.add(Bidirectional(LSTM(128, return_sequences=True)))\n        model.add(Dropout(0.3))\n        model.add(Bidirectional(GRU(128, return_sequences=True)))\n        model.add(Dropout(0.3))\n        model.add(BatchNormalization())\n        model.add(Bidirectional(GRU(64, return_sequences=True)))\n        model.add(Dropout(0.3))\n        model.add(Flatten())\n        model.add(Dense(128, activation='relu'))\n        model.add(Dropout(0.3))\n\n    if model_type == \"GRU\":\n        model.add(GRU(128, return_sequences=True))\n        model.add(Dropout(0.3))\n        model.add(GRU(64, return_sequences=True))\n        model.add(Dropout(0.3))\n        model.add(BatchNormalization())\n        model.add(GRU(32, return_sequences=True))\n        model.add(Dropout(0.3))\n        model.add(Flatten())\n        model.add(Dense(128, activation='relu'))\n        model.add(Dropout(0.3))\n\n    # For GRU\n    # model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0001), metrics=['accuracy'])\n    # model.fit(X_train, y_train, epochs=40, batch_size=32, validation_data=(X_dev, y_dev), verbose=1)\n    # return model\n\n    # For CNN, BiLSTM+CNN, BiLSTM+BiGRU\n    model.add(Dense(128, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(6, activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0001), metrics=['accuracy'])\n\n    # Train model\n    model.fit(X_train, y_train, epochs=45, batch_size=32, validation_data=(X_dev, y_dev), verbose=1)\n    return model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load datasets, train models, and make predictions for all languages\ndef solver_function():\n    results = {}\n    max_length = 100\n    vocab_size = 10000\n\n    for lang in languages:\n        print(f\"\\n Processing {lang}..\")\n        # Load datasets\n        train_path = f\"train_path_{lang}.csv\"\n        dev_path = f\"validation_path_{lang}.csv\"\n        test_path = f\"test_path_{lang}.csv\"\n\n        if not (os.path.exists(train_path) and os.path.exists(dev_path) and os.path.exists(test_path)):\n            print(f\"Missing dataset for {lang}, processing...\")\n            continue\n\n        train_df = pd.read_csv(train_path)\n        dev_df = pd.read_csv(dev_path)\n        test_df = pd.read_csv(test_path)\n\n        # Preprocess text\n        train_df['text'] = train_df['text'].apply(lambda x: preprocess_text(str(x), lang))\n        dev_df['text'] = dev_df['text'].apply(lambda x: preprocess_text(str(x), lang))\n        test_df['text'] = test_df['text'].apply(lambda x: preprocess_text(str(x), lang))\n\n        # Extract labels\n        emotion_labels = [\"anger\", \"disgust\", \"fear\", \"joy\", \"sadness\", \"surprise\"]\n        y_train = train_df[emotion_labels].values\n        y_dev = dev_df[emotion_labels].values\n\n        # Tokenization & Padding\n        tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n        tokenizer.fit_on_texts(train_df['text'])\n\n        X_train = pad_sequences(tokenizer.texts_to_sequences(train_df['text']), maxlen=max_length, padding='post')\n        X_dev = pad_sequences(tokenizer.texts_to_sequences(dev_df['text']), maxlen=max_length, padding='post')\n        X_test = pad_sequences(tokenizer.texts_to_sequences(test_df['text']), maxlen=max_length, padding='post')\n\n        # Train and predict using all DL models\n        models = [\"CNN\", \"BiLSTM+CNN\", \"BiLSTM+BiGRU\"]\n        # models = [\"GRU\"]\n        lang_results = {}\n\n        for model_name in models:\n            print(f\"\\nðŸ”¹ Training {model_name} for {lang}...\")\n            model = train_model(X_train, y_train, X_dev, y_dev, model_type=model_name)\n            # Make predictions\n            y_pred = model.predict(X_test)\n            predictions = (y_pred > 0.5).astype(int)\n            # Save predictions with text column\n            pred_df = test_df[['id', 'text']].copy()\n            pred_df[emotion_labels] = predictions\n            pred_df.to_csv(f\"{lang}_{model_name}_predictions.csv\", index=False)\n            lang_results[model_name] = pred_df\n\n        results[lang] = lang_results\n\n    return results","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Driver function\nif __name__ == \"__main__\":\n    predictions = solver_function()\n    print(\"\\n Predictions saved!!!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}